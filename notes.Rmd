---
title: Strain-level SCRS classification analysis (26 classes)
author: Research notes
date: October 2018
output:
  html_document:
    toc: true
    toc_float: true
    toc_depth: 3
---


# Data overview

## Preprocess

Raw data acquired is in Horiba format (encoded, maybe compressed/encrypted).
Data transformed by Dongqi using Horiba's LabSpec 6.
In three datasets:

* Exponential phase: 25 strains, 1440 samples
* Platform P1: 26 strains, 1392 samples
* Platform P2: 26 strains, 1407 samples
* TOTAL: 26 strains, 4239 samples

<font color="#FF0000"><b>!NOTE!</b></font> Non-essential changes made to the dataset:

* EXPONENT1-50/<font color="#FF0000">18-585/18-585</font> (containing redundant files as EXPONENT1-50/<font color="#FF0000">18-585</font>) is omitted
* PLATFORM1-50/18<font color="#FF0000">\_</font>3 renamed to PLATFORM1-50/18<font color="#FF0000">-</font>3

Check dimensionality:

```bash
$ find -name '*.txt' -exec wc -l {} \; | cut -f 1 -d ' ' | sort | uniq
```

All files have 1024 dimensions. Now check the number of samples:

```bash
$ ./script/hist_labels.py
```

![Number of samples in each dataset](./image/hist_labels.png)

The exponential phase has no strain "18-4".
Note all three datasets have roughly balanced classes.
But each class is relatively small due to the number of classes.
The SCRS were normalized by "l2", mean scaled by the root square sum.

Now it is good to explore the entire shape of data before doing analysis:
```bash
for file in {EXPONENT1-50,PLATFORM1-50,PLATFORM2-50}; do
	python3 ./script/data_pca.py \
		--data ./data/$file".normalized_l2.data.tsv" \
		--meta ./data/$file".normalized_l2.meta.tsv"
done
```

![PCA of exponential phase](./image/EXPONENT1-50.normalized_l2.data.tsv.pca.png)

![PCA of platform phase 1](./image/PLATFORM1-50.normalized_l2.data.tsv.pca.png)

![PCA of platform phase 2](./image/PLATFORM2-50.normalized_l2.data.tsv.pca.png)


# Exploratory 1

## Gaussian Naive Bayes (GNB)

Try GNB `(sklearn.naive_bayes.GaussianNB)`.
10-fold cross-validation, each class evenly splited into each sets.

```bash
classifier="gnb"
for file in {EXPONENT1-50,PLATFORM1-50,PLATFORM2-50}; do
	python3 ./script/t1_methods.py \
		--data ./data/$file".normalized_l2.data.tsv" \
		--meta ./data/$file".normalized_l2.meta.tsv" \
		--classifier $classifier
	for dimred in {pca,lda}; do
		for ndims in {20,26,40}; do
			python3 ./script/t1_methods.py \
				--data ./data/$file".normalized_l2.data.tsv" \
				--meta ./data/$file".normalized_l2.meta.tsv" \
				--classifier $classifier \
				--dim-reduc $dimred --reduce-dims-to $ndims
		done
	done
done
```

### GNB: w/o dimension reduction

#### Exponential phase

![](./image/EXPONENT1-50.normalized_l2.data.tsv.gnb.dr_none_none.10_fold.png)

#### Platform phase 1

![](./image/PLATFORM1-50.normalized_l2.data.tsv.gnb.dr_none_none.10_fold.png)

#### Platform phase 2

![](./image/PLATFORM2-50.normalized_l2.data.tsv.gnb.dr_none_none.10_fold.png)

### GNB: dimension reduction: PCA (26)

#### Exponential phase

![](./image/EXPONENT1-50.normalized_l2.data.tsv.gnb.dr_pca_26.10_fold.png)

#### Platform phase 1

![](./image/PLATFORM1-50.normalized_l2.data.tsv.gnb.dr_pca_26.10_fold.png)

#### Platform phase 2

![](./image/PLATFORM2-50.normalized_l2.data.tsv.gnb.dr_pca_26.10_fold.png)

### GNB: dimension reduction: LDA (26)

#### Exponential phase

![](./image/EXPONENT1-50.normalized_l2.data.tsv.gnb.dr_lda_26.10_fold.png)

#### Platform phase 1

![](./image/PLATFORM1-50.normalized_l2.data.tsv.gnb.dr_lda_26.10_fold.png)

#### Platform phase 2

![](./image/PLATFORM2-50.normalized_l2.data.tsv.gnb.dr_lda_26.10_fold.png)



## Logistic Regression (LR)

Try LR `(sklearn.linear_model.LogsticRegression)`.
10-fold cross-validation, each class evenly splited into each sets.

```bash
classifier="lr"
for file in {EXPONENT1-50,PLATFORM1-50,PLATFORM2-50}; do
	python3 ./script/t1_methods.py \
		--data ./data/$file".normalized_l2.data.tsv" \
		--meta ./data/$file".normalized_l2.meta.tsv" \
		--classifier $classifier
	for dimred in {pca,lda}; do
		for ndims in {20,26,40}; do
			python3 ./script/t1_methods.py \
				--data ./data/$file".normalized_l2.data.tsv" \
				--meta ./data/$file".normalized_l2.meta.tsv" \
				--classifier $classifier \
				--dim-reduc $dimred --reduce-dims-to $ndims
		done
	done
done
```

### LR: w/o dimension reduction

#### Exponential phase

![](./image/EXPONENT1-50.normalized_l2.data.tsv.lr.dr_none_none.10_fold.png)

#### Platform phase 1

![](./image/PLATFORM1-50.normalized_l2.data.tsv.lr.dr_none_none.10_fold.png)

#### Platform phase 2

![](./image/PLATFORM2-50.normalized_l2.data.tsv.lr.dr_none_none.10_fold.png)

### LR: dimension reduction: PCA (26)

#### Exponential phase

![](./image/EXPONENT1-50.normalized_l2.data.tsv.lr.dr_pca_26.10_fold.png)

#### Platform phase 1

![](./image/PLATFORM1-50.normalized_l2.data.tsv.lr.dr_pca_26.10_fold.png)

#### Platform phase 2

![](./image/PLATFORM2-50.normalized_l2.data.tsv.lr.dr_pca_26.10_fold.png)

### LR: dimension reduction: LDA (26)

#### Exponential phase

![](./image/EXPONENT1-50.normalized_l2.data.tsv.lr.dr_lda_26.10_fold.png)

#### Platform phase 1

![](./image/PLATFORM1-50.normalized_l2.data.tsv.lr.dr_lda_26.10_fold.png)

#### Platform phase 2

![](./image/PLATFORM2-50.normalized_l2.data.tsv.lr.dr_lda_26.10_fold.png)


## Linear Discriminant Analysis (LDA)

Try LDA `(sklearn.discriminant_analysis.LinearDiscriminantAnalysis)` as classifier.
10-fold cross-validation, each class evenly splited into each sets.

```bash
classifier="lda"
for file in {EXPONENT1-50,PLATFORM1-50,PLATFORM2-50}; do
	python3 ./script/t1_methods.py \
		--data ./data/$file".normalized_l2.data.tsv" \
		--meta ./data/$file".normalized_l2.meta.tsv" \
		--classifier $classifier
	for dimred in {pca,lda}; do
		for ndims in {20,26,40}; do
			python3 ./script/t1_methods.py \
				--data ./data/$file".normalized_l2.data.tsv" \
				--meta ./data/$file".normalized_l2.meta.tsv" \
				--classifier $classifier \
				--dim-reduc $dimred --reduce-dims-to $ndims
		done
	done
done
```

### LDA: w/o dimension reduction

#### Exponential phase

![](./image/EXPONENT1-50.normalized_l2.data.tsv.lda.dr_none_none.10_fold.png)

#### Platform phase 1

![](./image/PLATFORM1-50.normalized_l2.data.tsv.lda.dr_none_none.10_fold.png)

#### Platform phase 2

![](./image/PLATFORM2-50.normalized_l2.data.tsv.lda.dr_none_none.10_fold.png)

### LDA: dimension reduction: PCA (26)

#### Exponential phase

![](./image/EXPONENT1-50.normalized_l2.data.tsv.lda.dr_pca_26.10_fold.png)

#### Platform phase 1

![](./image/PLATFORM1-50.normalized_l2.data.tsv.lda.dr_pca_26.10_fold.png)

#### Platform phase 2

![](./image/PLATFORM2-50.normalized_l2.data.tsv.lda.dr_pca_26.10_fold.png)

### LDA: dimension reduction: LDA (26)

#### Exponential phase

![](./image/EXPONENT1-50.normalized_l2.data.tsv.lda.dr_lda_26.10_fold.png)

#### Platform phase 1

![](./image/PLATFORM1-50.normalized_l2.data.tsv.lda.dr_lda_26.10_fold.png)

#### Platform phase 2

![](./image/PLATFORM2-50.normalized_l2.data.tsv.lda.dr_lda_26.10_fold.png)


# Exploratory 2

## Linear Support Vector Machine (SVM)

Try SVM `(sklearn.svm.LinearSVM)`.
10-fold cross-validation, each class evenly splited into each sets.

```bash
classifier="svm_lin"
for file in {EXPONENT1-50,PLATFORM1-50,PLATFORM2-50}; do
	python3 ./script/t1_methods.py \
		--data ./data/$file".normalized_l2.data.tsv" \
		--meta ./data/$file".normalized_l2.meta.tsv" \
		--classifier $classifier
done
```

### SVM: w/o dimension reduction

#### Exponential phase

![](./image/EXPONENT1-50.normalized_l2.data.tsv.svm_lin.dr_none_none.10_fold.png)

#### Platform phase 1

![](./image/PLATFORM1-50.normalized_l2.data.tsv.svm_lin.dr_none_none.10_fold.png)

#### Platform phase 2

![](./image/PLATFORM2-50.normalized_l2.data.tsv.svm_lin.dr_none_none.10_fold.png)

### SVM: dimension reduction: PCA (26)

#### Exponential phase

![](./image/EXPONENT1-50.normalized_l2.data.tsv.svm_lin.dr_pca_26.10_fold.png)

#### Platform phase 1

![](./image/PLATFORM1-50.normalized_l2.data.tsv.svm_lin.dr_pca_26.10_fold.png)

#### Platform phase 2

![](./image/PLATFORM2-50.normalized_l2.data.tsv.svm_lin.dr_pca_26.10_fold.png)

### SVM: dimension reduction: LDA (26)

#### Exponential phase

![](./image/EXPONENT1-50.normalized_l2.data.tsv.svm_lin.dr_lda_26.10_fold.png)

#### Platform phase 1

![](./image/PLATFORM1-50.normalized_l2.data.tsv.svm_lin.dr_lda_26.10_fold.png)

#### Platform phase 2

![](./image/PLATFORM2-50.normalized_l2.data.tsv.svm_lin.dr_lda_26.10_fold.png)


## Kernel Support Vector Machine (SVM + RBF kernel)

Try SVM `(sklearn.svm.SVM(kernel = "rbf", gamma = <median Euclidean training set>))`.
10-fold cross-validation, each class evenly splited into each sets.

```bash
classifier="svm_lrbf"
for file in {EXPONENT1-50,PLATFORM1-50,PLATFORM2-50}; do
	python3 ./script/t1_methods.py \
		--data ./data/$file".normalized_l2.data.tsv" \
		--meta ./data/$file".normalized_l2.meta.tsv" \
		--classifier $classifier
done
```

### KSVM(RBF): w/o dimension reduction

#### Exponential phase

![](./image/EXPONENT1-50.normalized_l2.data.tsv.svm_rbf.dr_none_none.10_fold.png)

#### Platform phase 1

![](./image/PLATFORM1-50.normalized_l2.data.tsv.svm_rbf.dr_none_none.10_fold.png)

#### Platform phase 2

![](./image/PLATFORM2-50.normalized_l2.data.tsv.svm_rbf.dr_none_none.10_fold.png)

### KSVM(RBF): dimension reduction: PCA (26)

#### Exponential phase

![](./image/EXPONENT1-50.normalized_l2.data.tsv.svm_rbf.dr_pca_26.10_fold.png)

#### Platform phase 1

![](./image/PLATFORM1-50.normalized_l2.data.tsv.svm_rbf.dr_pca_26.10_fold.png)

#### Platform phase 2

![](./image/PLATFORM2-50.normalized_l2.data.tsv.svm_rbf.dr_pca_26.10_fold.png)

### KSVM(RBF): dimension reduction: LDA (26)

#### Exponential phase

![](./image/EXPONENT1-50.normalized_l2.data.tsv.svm_rbf.dr_lda_26.10_fold.png)

#### Platform phase 1

![](./image/PLATFORM1-50.normalized_l2.data.tsv.svm_rbf.dr_lda_26.10_fold.png)

#### Platform phase 2

![](./image/PLATFORM2-50.normalized_l2.data.tsv.svm_rbf.dr_lda_26.10_fold.png)


# HSIC Dimenstion Reduction (26)

As known ground truth, use `--num-clusters=26 --reduce-dims-to=26`.


```bash
for classifier in {gnb,lr,lda,svm_lin,svm_rbf}; do
	for file in {EXPONENT1-50,PLATFORM1-50,PLATFORM2-50}; do
		python3 ./script/t1_methods.py \
			--data ./data/$file".normalized_l2.data.tsv" \
			--meta ./data/$file".normalized_l2.meta.tsv" \
			--classifier $classifier \
			--dim-reduc lsdr --reduce-dims-to 26
	done
done
```

## Gaussian Naive Bayes (GNB)

### Exponential phase

![](./image/EXPONENT1-50.normalized_l2.data.tsv.gnb.dr_lsdr_26.10_fold.png)

### Platform phase 1

![](./image/PLATFORM1-50.normalized_l2.data.tsv.gnb.dr_lsdr_26.10_fold.png)

### Platform phase 2

![](./image/PLATFORM2-50.normalized_l2.data.tsv.gnb.dr_lsdr_26.10_fold.png)


## Logistic Regression (LR)

### Exponential phase

![](./image/EXPONENT1-50.normalized_l2.data.tsv.lr.dr_lsdr_26.10_fold.png)

### Platform phase 1

![](./image/PLATFORM1-50.normalized_l2.data.tsv.lr.dr_lsdr_26.10_fold.png)

### Platform phase 2

![](./image/PLATFORM2-50.normalized_l2.data.tsv.lr.dr_lsdr_26.10_fold.png)


## Linear Discriminant Analysis (LDA)

### Exponential phase

![](./image/EXPONENT1-50.normalized_l2.data.tsv.lda.dr_lsdr_26.10_fold.png)

### Platform phase 1

![](./image/PLATFORM1-50.normalized_l2.data.tsv.lda.dr_lsdr_26.10_fold.png)

### Platform phase 2

![](./image/PLATFORM2-50.normalized_l2.data.tsv.lda.dr_lsdr_26.10_fold.png)


## Linear Support Vector Machine (SVM)

### Exponential phase

![](./image/EXPONENT1-50.normalized_l2.data.tsv.svm_lin.dr_lsdr_26.10_fold.png)

### Platform phase 1

![](./image/PLATFORM1-50.normalized_l2.data.tsv.svm_lin.dr_lsdr_26.10_fold.png)

### Platform phase 2

![](./image/PLATFORM2-50.normalized_l2.data.tsv.svm_lin.dr_lsdr_26.10_fold.png)


## Kernel Support Vector Machine (SVM + RBF kernel)

### Exponential phase

![](./image/EXPONENT1-50.normalized_l2.data.tsv.svm_rbf.dr_lsdr_26.10_fold.png)

### Platform phase 1

![](./image/PLATFORM1-50.normalized_l2.data.tsv.svm_rbf.dr_lsdr_26.10_fold.png)

### Platform phase 2

![](./image/PLATFORM2-50.normalized_l2.data.tsv.svm_rbf.dr_lsdr_26.10_fold.png)




# NOTES

## 2018-12-28

* In cross-validataion, dimension reduction on full dataset then split, or split first then do on training set only? <b>A: should do only on training sets (i.e. consider as a part of training).</b>


## 2019-01-07

Revised process:

* dimension reduction on training set (this trains a dimensionality reducer)
* dimension reduction on test set (using the reducer trained above)
* train the classifier using <b>dim-reduced</b> training set
* predict labels of <b>dim-reduced</b> test set using the trained classifier
* evaluate classifier performance

It is suggested, transform test set using LSDR need to scale the test set first;
i.e. `sklearn.preprocessing.scale(X_test)`.
It seems the former nice results were overfitting.
But, the new results seems outcompeted by traditional method LDA-PCA(26).
Reasons unknown.

<font color="#FF0000">ABOUT SVM USED</font>

Checked the `sklearn.svm` documentation,

* Linear SVM (`sklearn.svm.LinearSVC(multi_class="ovr")`) is "one-over-rest" scheme for multiclass classifications
* General SVM (`sklearn.svm.SVC(kernel="rbf")`) is "one-to-one" scheme for multiclass classifications; need manual implementation for "one-over-rest"


## 2019-01-09

LSDR only:

* entire dataset is scaled before split into training/test sets

This improved a little performance than scale training set and test set separately.
However, the traditional LDA-PCA(26) still seems the best.
Reproduce the overfitting results (LSDR first, then traditional methods w/o dim-reduction) succesful.
No evidence of coding error found.