---
title: Strain-level SCRS classification analysis (26 classes)
author: Research notes
date: October 2018
output:
  html_document:
    toc: true
    toc_float: true
    toc_depth: 3
---


# Data overview

## Preprocess

Raw data acquired is in Horiba format (encoded, maybe compressed/encrypted).
Data transformed by Dongqi using Horiba's LabSpec 6.
In three datasets:

* Exponential phase: 25 strains, 1440 samples
* Platform P1: 26 strains, 1392 samples
* Platform P2: 26 strains, 1407 samples
* TOTAL: 26 strains, 4239 samples

<font color="#FF0000"><b>!NOTE!</b></font> Non-essential changes made to the dataset:

* EXPONENT1-50/<font color="#FF0000">18-585/18-585</font> (containing redundant files as EXPONENT1-50/<font color="#FF0000">18-585</font>) is omitted
* PLATFORM1-50/18<font color="#FF0000">\_</font>3 renamed to PLATFORM1-50/18<font color="#FF0000">-</font>3

Check dimensionality:

```bash
$ find -name '*.txt' -exec wc -l {} \; | cut -f 1 -d ' ' | sort | uniq
```

All files have 1024 dimensions. Now check the number of samples:

```bash
$ ./script/hist_labels.py
```

![Number of samples in each dataset](./image/hist_labels.png)

The exponential phase has no strain "18-4".
Note all three datasets have roughly balanced classes.
But each class is relatively small due to the number of classes.
The SCRS were normalized by "l2", mean scaled by the root square sum.

Now it is good to explore the entire shape of data before doing analysis:
```bash
for file in {EXPONENT1-50,PLATFORM1-50,PLATFORM2-50}; do
	python3 ./script/data_pca.py \
		--data ./data/$file".normalized_l2.data.tsv" \
		--meta ./data/$file".normalized_l2.meta.tsv"
done
```

![PCA of exponential phase](./image/EXPONENT1-50.normalized_l2.data.tsv.pca.png)

![PCA of platform phase 1](./image/PLATFORM1-50.normalized_l2.data.tsv.pca.png)

![PCA of platform phase 2](./image/PLATFORM2-50.normalized_l2.data.tsv.pca.png)


# Exploratory 1

## Gaussian Naive Bayes (GNB)

Try GNB `(sklearn.naive_bayes.GaussianNB)`.
10-fold cross-validation, each class evenly splited into each sets.

```bash
classifier="gnb"
for file in {EXPONENT1-50,PLATFORM1-50,PLATFORM2-50}; do
	python3 ./script/t1_methods.py \
		--data ./data/$file".normalized_l2.data.tsv" \
		--meta ./data/$file".normalized_l2.meta.tsv" \
		--classifier $classifier
	for dimred in {pca,lda}; do
		for ndims in {20,26,40}; do
			python3 ./script/t1_methods.py \
				--data ./data/$file".normalized_l2.data.tsv" \
				--meta ./data/$file".normalized_l2.meta.tsv" \
				--classifier $classifier \
				--dim-reduc $dimred --reduce-dims-to $ndims
		done
	done
done
```

### GNB: w/o dimension reduction

#### Exponential phase

![](./image/EXPONENT1-50.normalized_l2.data.tsv.gnb.dr_none_none.10_fold.png)

#### Platform phase 1

![](./image/PLATFORM1-50.normalized_l2.data.tsv.gnb.dr_none_none.10_fold.png)

#### Platform phase 2

![](./image/PLATFORM2-50.normalized_l2.data.tsv.gnb.dr_none_none.10_fold.png)

### GNB: dimension reduction: PCA (26)

#### Exponential phase

![](./image/EXPONENT1-50.normalized_l2.data.tsv.gnb.dr_pca_26.10_fold.png)

#### Platform phase 1

![](./image/PLATFORM1-50.normalized_l2.data.tsv.gnb.dr_pca_26.10_fold.png)

#### Platform phase 2

![](./image/PLATFORM2-50.normalized_l2.data.tsv.gnb.dr_pca_26.10_fold.png)

### GNB: dimension reduction: LDA (26)

#### Exponential phase

![](./image/EXPONENT1-50.normalized_l2.data.tsv.gnb.dr_lda_26.10_fold.png)

#### Platform phase 1

![](./image/PLATFORM1-50.normalized_l2.data.tsv.gnb.dr_lda_26.10_fold.png)

#### Platform phase 2

![](./image/PLATFORM2-50.normalized_l2.data.tsv.gnb.dr_lda_26.10_fold.png)



## Logistic Regression (LR)

Try LR `(sklearn.linear_model.LogsticRegression)`.
10-fold cross-validation, each class evenly splited into each sets.

```bash
classifier="lr"
for file in {EXPONENT1-50,PLATFORM1-50,PLATFORM2-50}; do
	python3 ./script/t1_methods.py \
		--data ./data/$file".normalized_l2.data.tsv" \
		--meta ./data/$file".normalized_l2.meta.tsv" \
		--classifier $classifier
	for dimred in {pca,lda}; do
		for ndims in {20,26,40}; do
			python3 ./script/t1_methods.py \
				--data ./data/$file".normalized_l2.data.tsv" \
				--meta ./data/$file".normalized_l2.meta.tsv" \
				--classifier $classifier \
				--dim-reduc $dimred --reduce-dims-to $ndims
		done
	done
done
```

### LR: w/o dimension reduction

#### Exponential phase

![](./image/EXPONENT1-50.normalized_l2.data.tsv.lr.dr_none_none.10_fold.png)

#### Platform phase 1

![](./image/PLATFORM1-50.normalized_l2.data.tsv.lr.dr_none_none.10_fold.png)

#### Platform phase 2

![](./image/PLATFORM2-50.normalized_l2.data.tsv.lr.dr_none_none.10_fold.png)

### LR: dimension reduction: PCA (26)

#### Exponential phase

![](./image/EXPONENT1-50.normalized_l2.data.tsv.lr.dr_pca_26.10_fold.png)

#### Platform phase 1

![](./image/PLATFORM1-50.normalized_l2.data.tsv.lr.dr_pca_26.10_fold.png)

#### Platform phase 2

![](./image/PLATFORM2-50.normalized_l2.data.tsv.lr.dr_pca_26.10_fold.png)

### LR: dimension reduction: LDA (26)

#### Exponential phase

![](./image/EXPONENT1-50.normalized_l2.data.tsv.lr.dr_lda_26.10_fold.png)

#### Platform phase 1

![](./image/PLATFORM1-50.normalized_l2.data.tsv.lr.dr_lda_26.10_fold.png)

#### Platform phase 2

![](./image/PLATFORM2-50.normalized_l2.data.tsv.lr.dr_lda_26.10_fold.png)


## Linear Discriminant Analysis (LDA)

Try LDA `(sklearn.discriminant_analysis.LinearDiscriminantAnalysis)` as classifier.
10-fold cross-validation, each class evenly splited into each sets.

```bash
classifier="lda"
for file in {EXPONENT1-50,PLATFORM1-50,PLATFORM2-50}; do
	python3 ./script/t1_methods.py \
		--data ./data/$file".normalized_l2.data.tsv" \
		--meta ./data/$file".normalized_l2.meta.tsv" \
		--classifier $classifier
	for dimred in {pca,lda}; do
		for ndims in {20,26,40}; do
			python3 ./script/t1_methods.py \
				--data ./data/$file".normalized_l2.data.tsv" \
				--meta ./data/$file".normalized_l2.meta.tsv" \
				--classifier $classifier \
				--dim-reduc $dimred --reduce-dims-to $ndims
		done
	done
done
```

### LDA: w/o dimension reduction

#### Exponential phase

![](./image/EXPONENT1-50.normalized_l2.data.tsv.lda.dr_none_none.10_fold.png)

#### Platform phase 1

![](./image/PLATFORM1-50.normalized_l2.data.tsv.lda.dr_none_none.10_fold.png)

#### Platform phase 2

![](./image/PLATFORM2-50.normalized_l2.data.tsv.lda.dr_none_none.10_fold.png)

### LDA: dimension reduction: PCA (26)

#### Exponential phase

![](./image/EXPONENT1-50.normalized_l2.data.tsv.lda.dr_pca_26.10_fold.png)

#### Platform phase 1

![](./image/PLATFORM1-50.normalized_l2.data.tsv.lda.dr_pca_26.10_fold.png)

#### Platform phase 2

![](./image/PLATFORM2-50.normalized_l2.data.tsv.lda.dr_pca_26.10_fold.png)

### LDA: dimension reduction: LDA (26)

#### Exponential phase

![](./image/EXPONENT1-50.normalized_l2.data.tsv.lda.dr_lda_26.10_fold.png)

#### Platform phase 1

![](./image/PLATFORM1-50.normalized_l2.data.tsv.lda.dr_lda_26.10_fold.png)

#### Platform phase 2

![](./image/PLATFORM2-50.normalized_l2.data.tsv.lda.dr_lda_26.10_fold.png)


# Exploratory 2

## Linear Support Vector Machine (SVM)

Try SVM `(sklearn.svm.LinearSVM)`.
10-fold cross-validation, each class evenly splited into each sets.

```bash
classifier="svm_lin"
for file in {EXPONENT1-50,PLATFORM1-50,PLATFORM2-50}; do
	python3 ./script/t1_methods.py \
		--data ./data/$file".normalized_l2.data.tsv" \
		--meta ./data/$file".normalized_l2.meta.tsv" \
		--classifier $classifier
	for dimred in {pca,lda}; do
		for ndims in {20,26,40}; do
			python3 ./script/t1_methods.py \
				--data ./data/$file".normalized_l2.data.tsv" \
				--meta ./data/$file".normalized_l2.meta.tsv" \
				--classifier $classifier \
				--dim-reduc $dimred --reduce-dim-to $ndims
		done
	done
done
```

### SVM: w/o dimension reduction

#### Exponential phase

![](./image/EXPONENT1-50.normalized_l2.data.tsv.svm_lin.dr_none_none.10_fold.png)

#### Platform phase 1

![](./image/PLATFORM1-50.normalized_l2.data.tsv.svm_lin.dr_none_none.10_fold.png)

#### Platform phase 2

![](./image/PLATFORM2-50.normalized_l2.data.tsv.svm_lin.dr_none_none.10_fold.png)

### SVM: dimension reduction: PCA (26)

#### Exponential phase

![](./image/EXPONENT1-50.normalized_l2.data.tsv.svm_lin.dr_pca_26.10_fold.png)

#### Platform phase 1

![](./image/PLATFORM1-50.normalized_l2.data.tsv.svm_lin.dr_pca_26.10_fold.png)

#### Platform phase 2

![](./image/PLATFORM2-50.normalized_l2.data.tsv.svm_lin.dr_pca_26.10_fold.png)

### SVM: dimension reduction: LDA (26)

#### Exponential phase

![](./image/EXPONENT1-50.normalized_l2.data.tsv.svm_lin.dr_lda_26.10_fold.png)

#### Platform phase 1

![](./image/PLATFORM1-50.normalized_l2.data.tsv.svm_lin.dr_lda_26.10_fold.png)

#### Platform phase 2

![](./image/PLATFORM2-50.normalized_l2.data.tsv.svm_lin.dr_lda_26.10_fold.png)


## Kernel Support Vector Machine (SVM + RBF kernel)

Try SVM `(sklearn.svm.SVM(kernel = "rbf", gamma = <median Euclidean training set>))`.
10-fold cross-validation, each class evenly splited into each sets.

```bash
classifier="svm_rbf"
for file in {EXPONENT1-50,PLATFORM1-50,PLATFORM2-50}; do
	python3 ./script/t1_methods.py \
		--data ./data/$file".normalized_l2.data.tsv" \
		--meta ./data/$file".normalized_l2.meta.tsv" \
		--classifier $classifier
	for dimred in {pca,lda}; do
		for ndims in {20,26,40}; do
			python3 ./script/t1_methods.py \
				--data ./data/$file".normalized_l2.data.tsv" \
				--meta ./data/$file".normalized_l2.meta.tsv" \
				--classifier $classifier \
				--dim-reduc $dimred --reduce-dim-to $ndims
		done
	done
done
```

### KSVM(RBF): w/o dimension reduction

#### Exponential phase

![](./image/EXPONENT1-50.normalized_l2.data.tsv.svm_rbf.dr_none_none.10_fold.png)

#### Platform phase 1

![](./image/PLATFORM1-50.normalized_l2.data.tsv.svm_rbf.dr_none_none.10_fold.png)

#### Platform phase 2

![](./image/PLATFORM2-50.normalized_l2.data.tsv.svm_rbf.dr_none_none.10_fold.png)

### KSVM(RBF): dimension reduction: PCA (26)

#### Exponential phase

![](./image/EXPONENT1-50.normalized_l2.data.tsv.svm_rbf.dr_pca_26.10_fold.png)

#### Platform phase 1

![](./image/PLATFORM1-50.normalized_l2.data.tsv.svm_rbf.dr_pca_26.10_fold.png)

#### Platform phase 2

![](./image/PLATFORM2-50.normalized_l2.data.tsv.svm_rbf.dr_pca_26.10_fold.png)

### KSVM(RBF): dimension reduction: LDA (26)

#### Exponential phase

![](./image/EXPONENT1-50.normalized_l2.data.tsv.svm_rbf.dr_lda_26.10_fold.png)

#### Platform phase 1

![](./image/PLATFORM1-50.normalized_l2.data.tsv.svm_rbf.dr_lda_26.10_fold.png)

#### Platform phase 2

![](./image/PLATFORM2-50.normalized_l2.data.tsv.svm_rbf.dr_lda_26.10_fold.png)


# HSIC Dimenstion Reduction (26)

As known ground truth, use `--num-clusters=26 --reduce-dims-to=26`.


```bash
for classifier in {gnb,lr,lda,svm_lin,svm_rbf}; do
	for file in {EXPONENT1-50,PLATFORM1-50,PLATFORM2-50}; do
		python3 ./script/t1_methods.py \
			--data ./data/$file".normalized_l2.data.tsv" \
			--meta ./data/$file".normalized_l2.meta.tsv" \
			--classifier $classifier \
			--dim-reduc lsdr --reduce-dims-to 26
	done
done
```

## Gaussian Naive Bayes (GNB)

### Exponential phase

![](./image/EXPONENT1-50.normalized_l2.data.tsv.gnb.dr_lsdr_26.10_fold.png)

### Platform phase 1

![](./image/PLATFORM1-50.normalized_l2.data.tsv.gnb.dr_lsdr_26.10_fold.png)

### Platform phase 2

![](./image/PLATFORM2-50.normalized_l2.data.tsv.gnb.dr_lsdr_26.10_fold.png)


## Logistic Regression (LR)

### Exponential phase

![](./image/EXPONENT1-50.normalized_l2.data.tsv.lr.dr_lsdr_26.10_fold.png)

### Platform phase 1

![](./image/PLATFORM1-50.normalized_l2.data.tsv.lr.dr_lsdr_26.10_fold.png)

### Platform phase 2

![](./image/PLATFORM2-50.normalized_l2.data.tsv.lr.dr_lsdr_26.10_fold.png)


## Linear Discriminant Analysis (LDA)

### Exponential phase

![](./image/EXPONENT1-50.normalized_l2.data.tsv.lda.dr_lsdr_26.10_fold.png)

### Platform phase 1

![](./image/PLATFORM1-50.normalized_l2.data.tsv.lda.dr_lsdr_26.10_fold.png)

### Platform phase 2

![](./image/PLATFORM2-50.normalized_l2.data.tsv.lda.dr_lsdr_26.10_fold.png)


## Linear Support Vector Machine (SVM)

### Exponential phase

![](./image/EXPONENT1-50.normalized_l2.data.tsv.svm_lin.dr_lsdr_26.10_fold.png)

### Platform phase 1

![](./image/PLATFORM1-50.normalized_l2.data.tsv.svm_lin.dr_lsdr_26.10_fold.png)

### Platform phase 2

![](./image/PLATFORM2-50.normalized_l2.data.tsv.svm_lin.dr_lsdr_26.10_fold.png)


## Kernel Support Vector Machine (SVM + RBF kernel)

### Exponential phase

![](./image/EXPONENT1-50.normalized_l2.data.tsv.svm_rbf.dr_lsdr_26.10_fold.png)

### Platform phase 1

![](./image/PLATFORM1-50.normalized_l2.data.tsv.svm_rbf.dr_lsdr_26.10_fold.png)

### Platform phase 2

![](./image/PLATFORM2-50.normalized_l2.data.tsv.svm_rbf.dr_lsdr_26.10_fold.png)


# Results overview

## Average accuracy

![](./summary/EXPONENT1-50.normalized_l2.data.tsv.summary.accuracy.png)

![](./summary/PLATFORM1-50.normalized_l2.data.tsv.summary.accuracy.png)

![](./summary/PLATFORM2-50.normalized_l2.data.tsv.summary.accuracy.png)

## Train/test accuracy w.r.t. dimension reduction

```bash
dims_test_dir="output/dims_test"
mkdir -p $dims_test_dir
for file in {EXPONENT1-50,PLATFORM1-50,PLATFORM2-50}; do
	mkdir -p $dims_test_dir/$file
	for dimred in {pca,lda,lsdr}; do
		mkdir -p $dims_test_dir/$file/$dimred
		#for model in {gnb,lr,lda,svm_lin,svm_rbf}; do
		for model in {svm_lin,svm_rbf}; do
			mkdir -p $dims_test_dir/$file/$dimred/$model
			for ndims in {1..30}; do
				python3 ./script/t1_methods.py \
					--data ./data/$file".normalized_l2.data.tsv" \
					--meta ./data/$file".normalized_l2.meta.tsv" \
					--classifier $model \
					--dim-reduc $dimred --reduce-dim-to $ndims \
					--output-txt-dir $dims_test_dir/$file/$dimred/$model \
					--output-png-dir $dims_test_dir/$file/$dimred/$model
			done > $dims_test_dir/$file/$dimred/$model/log
		done
	done
done
# plot
python3 ./script/plot_dims_test.py $dims_test_dir
```

### Exponential Phase

![](./image/EXPONENT1-50.dims_test.png)

### Platform phase 1

![](./image/PLATFORM1-50.dims_test.png)

### Platform phase 2

![](./image/PLATFORM2-50.dims_test.png)


# Stage 2: combined labels (phases, strains)

Original dataset were combined in two ways to get two new datasets:

* Clump all strains in each phase, resulting in a dataset with three phases as labels
* Clump all phases each strain, resulting in a dataset with 26 strains


## Result overview

## Average accuracy

![](./summary/COMBINED.phase.normalized_l2.data.tsv.summary.accuracy.png)

![](./summary/COMBINED.strain.normalized_l2.data.tsv.summary.accuracy.png)


### Three phases

![](./image/COMBINED.phase.dims_test.png)

### 26 strains

![](./image/COMBINED.strain.dims_test.png)


# NOTES

## 2018-12-28

* In cross-validataion, dimension reduction on full dataset then split, or split first then do on training set only? <b>A: should do only on training sets (i.e. consider as a part of training).</b>


## 2019-01-07

Revised process:

* dimension reduction on training set (this trains a dimensionality reducer)
* dimension reduction on test set (using the reducer trained above)
* train the classifier using <b>dim-reduced</b> training set
* predict labels of <b>dim-reduced</b> test set using the trained classifier
* evaluate classifier performance

It is suggested, transform test set using LSDR need to scale the test set first;
i.e. `sklearn.preprocessing.scale(X_test)`.
It seems the former nice results were overfitting.
But, the new results seems outcompeted by traditional method LDA-PCA(26).
Reasons unknown.

<font color="#FF0000">ABOUT SVM USED</font>

Checked the `sklearn.svm` documentation,

* Linear SVM (`sklearn.svm.LinearSVC(multi_class="ovr")`) is "one-over-rest" scheme for multiclass classifications
* General SVM (`sklearn.svm.SVC(kernel="rbf")`) is "one-to-one" scheme for multiclass classifications; need manual implementation for "one-over-rest"


## 2019-01-09

LSDR only:

* entire dataset is scaled before split into training/test sets

This improved a little performance than scale training set and test set separately.
However, the traditional LDA-PCA(26) still seems the best.
Reproduce the overfitting results (LSDR first, then traditional methods w/o dim-reduction) succesful.
No evidence of coding error found.


## 2019-01-11

<b>As a quick conclusion, the LSDR seems is overfitting, or, the data size is insufficient.</b>

This conclusion was made via comparison the training accuracy and testing accuracy.
In addition, <b>two different strategies</b> were tried, respectively:

(1) Split, then train dimension reduction on the training set ONLY, before transfrom the testing set.
The pseudocode is:

```
foreach in (splitted dataset) /* 10-fold cv */
{
    fit lsdr to (train_X, train_Y);
    train_X_dr <- dim-reduct on train_X;
    test_X_dr <- dim-reduct on test_X;
    /* classifier training */
    fit classifier to (train_X_dr, train_Y);
    /* train accuracy */
    pred_train_Y <- predict on train_X_dr;
    train_accuracy <- accuracy(train_Y, pred_train_Y);
    /* test accuracy */
    pred_test_Y <- predict on test_X_dr;
    test_accuracy <- accuracy(test_Y, pred_test_Y);
}
```

Results were:

```
data: data/EXPONENT1-50.normalized_l2.data.tsv
meta: data/EXPONENT1-50.normalized_l2.meta.tsv
classifier: lda
cv_folds: 10
permutation: random
dim_reduc: lsdr
reduce_dim_to: 25
training accuracy: 0.989
test accuracy: 0.614
training accuracy: 0.988
test accuracy: 0.678
training accuracy: 0.988
test accuracy: 0.637
training accuracy: 0.991
test accuracy: 0.646
training accuracy: 0.988
test accuracy: 0.615
training accuracy: 0.988
test accuracy: 0.539
training accuracy: 0.987
test accuracy: 0.552
training accuracy: 0.988
test accuracy: 0.592
training accuracy: 0.987
test accuracy: 0.697
training accuracy: 0.987
test accuracy: 0.643
```

(2) Train dimension reduction on whole dataset, then split data for 10-fold CV;
In each fold, keep the trained dimension reduction unchanged, while transforming
training and testing data separately.
The pseudocode is:

```
fit lsdr to (all_X, all_Y);
for each in (10-fold cv, splitted dataset)
{
    train_X_dr <- dim-reduct on train_X;
    test_X_dr <- dim-reduct on test_X;
    /* classifier training */
    fit classifier to (train_X_dr, train_Y);
    /* train accuracy */
    pred_train_Y <- predict on train_X_dr;
    train_accuracy <- accuracy(train_Y, pred_train_Y);
    /* test accuracy */
    pred_test_Y <- predict on test_X_dr;
    test_accuracy <- accuracy(test_Y, pred_test_Y);
}
```

Results were:

```
data: data/EXPONENT1-50.normalized_l2.data.tsv
meta: data/EXPONENT1-50.normalized_l2.meta.tsv
classifier: lda
cv_folds: 10
permutation: random
dim_reduc: lsdr
reduce_dim_to: 25
training accuracy: 0.985
test accuracy: 0.970
training accuracy: 0.985
test accuracy: 0.965
training accuracy: 0.985
test accuracy: 0.979
training accuracy: 0.987
test accuracy: 0.965
training accuracy: 0.985
test accuracy: 0.986
training accuracy: 0.985
test accuracy: 0.957
training accuracy: 0.987
test accuracy: 0.979
training accuracy: 0.985
test accuracy: 0.986
training accuracy: 0.986
test accuracy: 0.986
training accuracy: 0.984
test accuracy: 0.987
```

Note in (1) the classifier did constantly perfect on training set but pool on testing set;
This is precisely overfitting.
Meanwhile, training on the entire dataset generates far more better results.

In addition, added tabularization of the results for overview comparisons.


## 2019-01-15

Check the train/test accuracy for all combinations of dimension reduction and classifier methods.

In addition, time in training something as below:


Reduce dims    EXPONENT1    PLATFORM1    PLATFORM2
-----------    ---------    ---------    ---------
1              < 1 min      < 1 min      < 1 min
2              < 1 min      4 min        < 1 min
3              < 1 min      14 min       < 1 min
4              < 1 min      17 min       < 1 min
5              < 1 min      17 min       < 1 min
6              < 1 min      17 min       < 1 min
7              < 1 min      17 min       < 1 min
8              < 1 min      17 min       < 1 min
9              < 1 min      17 min       < 1 min
10             < 1 min      17 min       < 1 min
11             < 1 min      17 min       < 1 min
12             < 1 min      17 min       < 1 min
13             < 1 min      17 min       < 1 min
14             < 1 min      16 min       < 1 min
15             < 1 min      7 min        < 1 min
16             < 1 min      8 min        < 1 min
17             < 1 min      5 min        < 1 min
18             < 1 min      4 min        < 1 min
19             < 1 min      < 1 min      < 1 min
20             < 1 min      < 1 min      < 1 min
21             < 1 min      < 1 min      < 1 min
22             < 1 min      < 1 min      < 1 min
23             < 1 min      < 1 min      < 1 min
24             < 1 min      < 1 min      < 1 min
25             < 1 min      < 1 min      < 1 min
26             < 1 min      < 1 min      < 1 min
27             < 1 min      < 1 min      < 1 min
28             < 1 min      < 1 min      < 1 min
29             < 1 min      < 1 min      < 1 min
30             < 1 min      < 1 min      < 1 min


## 2019-01-17

Almost made conclusions:

1) PCA is least overfitting, LDA has worst overfitting;
2) To achive same level of accuracy, LSDR requires less dimensions than PCA (in general);
5) LSDR becomes overfitting when downgrades into more than 15 dimensions (in general), this overfitting is potentially due to the insufficient number of samples each class;
3) LDA overfitting is due to the high dimensionality in original data;
4) PCA (dimension reduction) + LDA (classifier) performs well in general;